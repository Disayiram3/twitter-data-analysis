{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ],
      "metadata": {
        "id": "P5KETZ-timVU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJTTLupclWyr",
        "outputId": "a06089d3-0227-45ff-d326-053424d4f9f8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.40.1)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.14.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "No model was supplied, defaulted to sshleifer/distilbart-cnn-12-6 and revision a4f8f3e (https://huggingface.co/sshleifer/distilbart-cnn-12-6).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "Your max_length is set to 150, but your input_length is only 23. Since this is a summarization task, where outputs shorter than the input are typically wanted, you might consider decreasing max_length manually, e.g. summarizer('...', max_length=11)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Summarized Text:   google annual developer conference fast approaching . come announcement new device showcase company software year come . google pixel fold will be unveiled at the Google annual conference .\n",
            "Original Article:  Google’s annual developer conference is fast approaching, and with it comes announcements of new devices that will showcase the company’s software for years to come. The Google Pixel Fold 2, which ma… [+4592 chars]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from transformers import pipeline\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import re\n",
        "\n",
        "# Make sure to install these dependencies or include installation commands in your Colab notebook\n",
        "!pip install transformers nltk\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Load the data\n",
        "data = pd.read_csv(\"/content/drive/MyDrive/GoogleColab/web social media anlysis and visualization/api_news_articles.csv\")\n",
        "\n",
        "# Preprocessing function to remove non-ASCII characters\n",
        "def preprocess_text(text):\n",
        "    # Remove non-ASCII characters\n",
        "    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n",
        "    # Remove placeholders like \"[+4592 chars]\"\n",
        "    text = re.sub(r\"\\[\\+\\d+\\schars\\]\", \"\", text)\n",
        "    # Lowercasing\n",
        "    text = text.lower()\n",
        "    # Remove non-alphabetic characters, excluding basic punctuation\n",
        "    text = re.sub(r'[^a-zA-Z\\s.,!?]', '', text, re.I|re.A)\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "    # Removing stopwords\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Lemmatization\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_text = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
        "    # Re-join tokens\n",
        "    return ' '.join(lemmatized_text)\n",
        "\n",
        "# Select the 13th article (index 12)\n",
        "try:\n",
        "    article = data.loc[12, 'content']  # Adjust index to 12 for the 13th article\n",
        "    cleaned_article = preprocess_text(article)\n",
        "except IndexError:\n",
        "    print(\"The specified index is out of range. Please check your dataset.\")\n",
        "    cleaned_article = \"\"\n",
        "\n",
        "# Initialize the summarization pipeline\n",
        "summarizer = pipeline(\"summarization\")\n",
        "\n",
        "# Perform summarization\n",
        "if cleaned_article:  # Check if the article contains text after preprocessing\n",
        "    summary = summarizer(cleaned_article, max_length=150, min_length=30, do_sample=False)\n",
        "    if summary:\n",
        "        summarized_text = summary[0]['summary_text']\n",
        "        print(\"Summarized Text: \", summarized_text)\n",
        "    else:\n",
        "        print(\"No summary was generated. Please check the content of the article.\")\n",
        "else:\n",
        "    print(\"No content to summarize.\")\n",
        "\n",
        "# Print the original article for comparison (if it exists)\n",
        "if 'article' in locals():\n",
        "    print(\"Original Article: \", article)\n"
      ]
    }
  ]
}